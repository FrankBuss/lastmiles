

lab$ sudo setcap cap_sys_admin+ep /home/dclarke/pgm/src/cuda/vector_add/vaddd
lab$ getcap /home/dclarke/pgm/src/cuda/vector_add/vaddd
/home/dclarke/pgm/src/cuda/vector_add/vaddd = cap_sys_admin+ep
lab$ 


nix$ 
nix$ dpkg-query -l | grep 'NVIDIA Performance Primitive' | cut -c1-130
ii  libnppc9.2:amd64                     9.2.148-7                           amd64        NVIDIA Performance Primitives core runti
ii  libnppial9.2:amd64                   9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppicc9.2:amd64                   9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppicom9.2:amd64                  9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppidei9.2:amd64                  9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppif9.2:amd64                    9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppig9.2:amd64                    9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppim9.2:amd64                    9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppist9.2:amd64                   9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppisu9.2:amd64                   9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnppitc9.2:amd64                   9.2.148-7                           amd64        NVIDIA Performance Primitives lib for Im
ii  libnpps9.2:amd64                     9.2.148-7                           amd64        NVIDIA Performance Primitives for signal
nix$ nvidia-smi
Sun Jan 26 21:18:08 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro K4200        On   | 00000000:01:00.0  On |                  N/A |
| 31%   45C    P8    24W / 110W |     95MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1035      G   /usr/lib/xorg/Xorg                            91MiB |
+-----------------------------------------------------------------------------+
nix$ nvcc --version 
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148
nix$ 

nix$ nvcc -ccbin g++ -I../include -m64 -gencode arch=compute_30,code=sm_30 -c -o vaddd.o vaddd.cu
nix$ nvcc -ccbin g++ -m64 -gencode arch=compute_30,code=sm_30 -o vaddd vaddd.o -lgomp
nix$ 



nix$ 
nix$ nvprof ./vaddd 

---------+---------+---------+---------+---------+---------+---------+--
==14521== NVPROF is profiling process 14521, command: ./vaddd
INFO : number of host CPUs:     2
INFO : number of CUDA devices:  1
     :    0: Quadro K4200
---------+---------+---------+---------+---------+---------+---------+--
INFO : Vector addition of 500000 double FP64 elements
     : Memory size is 4000000 bytes
INFO : Copy of vector A from host to device done.
INFO : Copy of vector B from host to device done.
INFO : CUDA kernel launch with 489 blocks of 1024 threads
INFO : vectorAdd done.
INFO : Copy result vector A from device to host done.
INFO : A + B correct within error epsilon =  1.000000000e-12
INFO : host memory free and we are done
==14521== Profiling application: ./vaddd
==14521== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   49.31%  1.2210ms         2  610.49us  610.37us  610.62us  [CUDA memcpy HtoD]
                   47.17%  1.1680ms         1  1.1680ms  1.1680ms  1.1680ms  [CUDA memcpy DtoH]
                    3.52%  87.071us         1  87.071us  87.071us  87.071us  vectorAdd(double const *, double const *, double*, int)
      API calls:   89.38%  49.645ms         3  16.548ms  110.27us  49.422ms  cudaMalloc
                    6.09%  3.3843ms         3  1.1281ms  510.99us  2.1864ms  cudaMemcpy
                    1.59%  884.10us         1  884.10us  884.10us  884.10us  cudaGetDeviceProperties
                    1.47%  814.57us        96  8.4850us     360ns  358.64us  cuDeviceGetAttribute
                    0.63%  350.48us         3  116.83us  91.443us  161.40us  cudaFree
                    0.59%  327.58us         1  327.58us  327.58us  327.58us  cuDeviceTotalMem
                    0.18%  101.96us         1  101.96us  101.96us  101.96us  cuDeviceGetName
                    0.05%  25.095us         1  25.095us  25.095us  25.095us  cudaLaunchKernel
                    0.01%  5.2350us         1  5.2350us  5.2350us  5.2350us  cuDeviceGetPCIBusId
                    0.01%  3.1080us         3  1.0360us     433ns  2.0350us  cuDeviceGetCount
                    0.00%  1.9060us         1  1.9060us  1.9060us  1.9060us  cudaGetDeviceCount
                    0.00%  1.8250us         2     912ns     460ns  1.3650us  cuDeviceGet
                    0.00%     224ns         1     224ns     224ns     224ns  cudaGetLastError
nix$ 

We may use the visual profiler also :

nix$ nvcc -ccbin g++ -I./inc -m64 -gencode arch=compute_30,code=sm_30 -c -o vaddd.o vaddd.cu
nix$ nvcc -ccbin g++ -m64 -gencode arch=compute_30,code=sm_30 -o vaddd vaddd.o -lgomp
nix$ 
nix$ pwd                                                       
/home/dclarke/pgm/src/cuda/experiment0
nix$ 
nix$ nvvp -vm /usr/lib/jvm/nvidia-java-8-openjdk-amd64/bin/java
nix$ 


--------------------------------------------------------



HOWEVER for a newer NVidia Quadro device we must use compute
capability ( NVidia sales language ) level 7.0

nix$ nvcc -ccbin g++ -I./inc -m64 -gencode arch=compute_70,code=sm_70 -c -o vaddd.o vaddd.cu
nix$ nvcc -ccbin g++ -m64 -gencode arch=compute_70,code=sm_70 -o vaddd vaddd.o -lgomp

That will NOT run on an older smaller K4200 : 

nix$ nvprof ./vaddd 

---------+---------+---------+---------+---------+---------+---------+--
==15035== NVPROF is profiling process 15035, command: ./vaddd
INFO : number of host CPUs:     2
INFO : number of CUDA devices:  1
     :    0: Quadro K4200
---------+---------+---------+---------+---------+---------+---------+--
INFO : Vector addition of 500000 double FP64 elements
     : Memory size is 4000000 bytes
INFO : Copy of vector A from host to device done.
INFO : Copy of vector B from host to device done.
INFO : CUDA kernel launch with 489 blocks of 1024 threads
FAIL : CUDA failed vectorAdd
FAIL : error no kernel image is available for execution on the device



A test on a Lenovo Laptop W541 model with a Quadro chip inside it : 

C_styx$ nvcc -ccbin g++ -I../include -m64 -gencode arch=compute_30,code=sm_30 -c -o vaddd.o vaddd.cu
C_styx$ nvcc -ccbin g++ -m64 -gencode arch=compute_30,code=sm_30 -o vaddd vaddd.o -lgomp
C_styx$ nvprof ./vaddd
-------------------------------------------------------------
        system name = Linux
          node name = styx
            release = 4.19.0-8-amd64
            version = #1 SMP Debian 4.19.98-1 (2020-01-26)
            machine = x86_64
          page size = 4096
       avail memory = 16402386944
                    = 16017956 kB
                    = 15642 MB
-------------------------------------------------------------
==1191== NVPROF is profiling process 1191, command: ./vaddd
INFO : number of host CPUs:     8
INFO : number of CUDA devices:  1
     :    0: Quadro K1100M
INFO : Vector addition of 80000000 double FP64 elements
     : Memory size of each array is 640000000 bytes
     : random data loaded 2560669233 nsecs   2.560669 secs
     : cudaMalloc(A) 92232777 nsecs  0.09223278 secs
     : cudaMalloc(B) 1562559 nsecs  0.001562559 secs
     : cudaMalloc(C) 1516225 nsecs  0.001516225 secs
INFO : Copy of vector A from host to device done.
     : cudaMemcpy() 181259496 nsecs  0.1812595 secs
INFO : Copy of vector B from host to device done.
     : cudaMemcpy() 198579680 nsecs  0.1985797 secs
INFO : CUDA kernel launch with 312500 blocks of 256 threads
INFO : vector_add done.
     : vector_add 111830 nsecs  0.00011183 secs
INFO : Copy result A from device to host done.
     : cudaMemcpy() 549964970 nsecs   0.549965 secs
INFO : A + B correct within error epsilon =  1.000000000e-12
     : epsilon 508040032 nsecs    0.50804 secs
INFO : host memory free and we are done
==1191== Profiling application: ./vaddd
==1191== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   53.39%  495.62ms         1  495.62ms  495.62ms  495.62ms  [CUDA memcpy DtoH]
                   40.85%  379.23ms         2  189.61ms  180.97ms  198.26ms  [CUDA memcpy HtoD]
                    5.77%  53.529ms         1  53.529ms  53.529ms  53.529ms  vector_add(double const *, double const *, double*, int)
      API calls:   90.21%  929.68ms         3  309.89ms  181.21ms  549.94ms  cudaMemcpy
                    9.24%  95.216ms         3  31.739ms  1.4795ms  92.195ms  cudaMalloc
                    0.36%  3.7149ms         3  1.2383ms  1.0956ms  1.4865ms  cudaFree
                    0.07%  772.49us         1  772.49us  772.49us  772.49us  cudaGetDeviceProperties
                    0.07%  757.13us        96  7.8860us     185ns  358.41us  cuDeviceGetAttribute
                    0.02%  205.12us         1  205.12us  205.12us  205.12us  cuDeviceTotalMem
                    0.01%  122.30us         1  122.30us  122.30us  122.30us  cuDeviceGetName
                    0.01%  71.094us         1  71.094us  71.094us  71.094us  cudaLaunchKernel
                    0.00%  5.7500us         1  5.7500us  5.7500us  5.7500us  cuDeviceGetPCIBusId
                    0.00%  2.0850us         3     695ns     312ns  1.1970us  cuDeviceGetCount
                    0.00%  1.5770us         1  1.5770us  1.5770us  1.5770us  cudaGetDeviceCount
                    0.00%  1.1380us         2     569ns     270ns     868ns  cuDeviceGet
                    0.00%     530ns         1     530ns     530ns     530ns  cudaGetLastError
C_styx$ 
C_styx$ echo '36k 1.0 53.529 1000 / 80000000 / p /pq' | dc 
.000000000669112500000000000000000000
1494516990.790039044256384389770031198042182742
C_styx$ 

        so 1.5 GFlop in a laptop 




